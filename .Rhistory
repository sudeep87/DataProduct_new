submit()
?chain
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students,sex,count,-grade)
students2
gather(students2,sex_class,count,-grade)
res<-gather(students2,sex_class,count,-grade)
res
separate()
?separate()
?separate
separate(data=res,col=sex_class,into=c("sex","class"))
submit()
students3
gather(students3,class,count,-(name:test))
gather(students3,class,grade,-(name:test))
gather(students3,class,grade,-(name:test))%>%separate(col=test,into=c(test_final,test_midterm))
gather(students3,class,grade,-(name:test))%>%separate(col=test,into=c("test_final","test_midterm"))
submit()
?spread
submit()
submit()
submit()
students3 %>%gather(class, grade, class1:class5, na.rm = TRUE)
submit()
submit()
submit()
reset()
reset()
students3 %>%gather(class, grade, class1:class5, na.rm = TRUE)
submit()
extract_numeric("class5")
submit()
students4
submit()
submit()
submit()
passed
failed
passed%>%mutate(status='passed')
passed<-passed%>%mutate(status="passed")
failed<-failed%>%mutate(status="failed")
?rbind_list
rbind_list(passed,failed)
sat
sat %>%
select(-contains(total)) %>%
gather(part_sex,count , -score_range)
?contains
??contains
?select
sat %>%select(-contains(total,ignore.case=FALSE)) %>%gather(part_sex,count , -score_range)
sat %>%select(-contains("total",ignore.case=FALSE)) %>%gather(part_sex,count , -score_range)
submit()
submit()
submit()
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", "12d56d4478db897db759")
myapp <- oauth_app("github", "12d56d4478db897db759","b5cc04b3708de5804550aaf45e4b92d58e8e74e7")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httpuv)
?httpuv
??httpuv
rm(list=ls())
oauth_endpoints("github")
myapp <- oauth_app("github", "12d56d4478db897db759","b5cc04b3708de5804550aaf45e4b92d58e8e74e7")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
install.packages("devtools")
library("devtools")
library(datasets)
data(cars)
ls()
cars
with(cars,plot(speed,dist))
library(lattice)
state<-data.frame(state.x77,region=state.region)
state
xyplot(Life.Exp~Income|region,data=state,layout=c(4,1))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(training$CompressiveStrength)
plot(training$CompressiveStrength,color)
names(training)
plot(training$CompressiveStrength,color="Age")
plot(training$CompressiveStrength,colour="Age")
plot(training$CompressiveStrength,colour=Age)
plot(training$CompressiveStrength,colour=training$Age)
plot(CompressiveStrength,colour=Age,data=training)
plot(CompressiveStrength,FlyAsh,colour=Age,data=training)
data(segmentationOriginal)
library(caret)
testIndex = createDataPartition(segmentationOriginal$Case, p = 0.50,list=FALSE)
training = segmentationOriginal[-testIndex,]
testing = segmentationOriginal[testIndex,]
set.seed(125)
names(training)
modfit<-train(Case~.,method="rpart",data=training)
print(modfit$finalmodel)
training<-training[!is.null(Case),]
training<-training[!is.null(training$Case),]
set.seed(125)
modfit<-train(Case~.,method="rpart",data=training)
print(modfit$finalmodel)
modfit<-train(Class~.,method="rpart",data=training)
print(modfit$finalmodel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
install.packages(rattle)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
install.packages("rattle")
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
library(rattle)
rattle()
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
rattle()
install.packages("rattle")
install.packages("rattle")
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata = train)
fancyRpartPlot(modFit$finalModel)
dff<-predict(modFit, newdata = train)
dff[TotalIntench2 == 23,000 & FiberWidthCh1 == 10 & PerimStatusCh1==2, ]
names(dff)
dff
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
model <- train(Area ~ ., data = olive, method = "rpart2")
library(caret)
model <- train(Area ~ ., data = olive, method = "rpart2")
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata = newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
aet.seed(13234)
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library(caret)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
a <- RandomForest(y ~ ., data = vowel.train, importance = FALSE)
ls(getNamespace("randomForest"), all.names=TRUE)
getAnywhere(predict.randomForest)
library(someRpackage)
install.packages("randomForest")
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library(randomForest)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
order(b)
b
b[order (b)]
b[order (b),]
order(b)
pnorm(70, mean = 80, sd = 10, lower.tail = FALSE)
pnorm(2, lower.tail = FALSE)
1-pnorm(70, mean = 80, sd = 10, lower.tail = FALSE)
qnorm(.95, mean = 1100, sd = 75)
qnorm(.10, mean = 1100, sd = 75)
qnorm(.10, mean = 1100, sd = 75.n=100)
qnorm(.10, mean = 1100, sd = 75,n=100)
qnorm(.95, mean = 11000, sd = (10/sqrt(100)))
qnorm(.95, mean = 1100, sd = (10/sqrt(100)))
setwd("~/data")
sData_complete <- read.csv(bzfile("stormData.csv.bz2"), strip.white = TRUE)
rm()
rm("stormdata.zip")
rm(stormdata.zip)
setwd("~/")
library(corrrplot)
install.packages(corrrplot)
install.packages("corrrplot")
library(caret)
library(corrplot)
library(kernlab)
library(knitr)
library(randomForest)
install.packages("corrplot")
library(corrplot)
install.packages("kernlab")
library(kernlab)
library(caret)
library(corrplot)
library(kernlab)
library(knitr)
library(randomForest)
opts_chunk$set(cache = FALSE)
# check if a data folder exists; if not then create one
if (!file.exists("data")) {dir.create("data")}
# file URL and destination file
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile1 <- "./data/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile2 <- "./data/pml-testing.csv"
# download the file and note the time
download.file(fileUrl1, destfile = destfile1)
download.file(fileUrl2, destfile = destfile2)
dateDownloaded <- date()
# read the csv file for training
data_training <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
# clean the data by removing columns with NAs etc
data_training_NAs <- apply(data_training, 2, function(x) {sum(is.na(x))})
data_training_clean <- data_training[,which(data_training_NAs == 0)]
# remove identifier columns such as name, timestamps etc
data_training_clean <- data_training_clean[8:length(data_training_clean)]
# split the cleaned testing data into training and cross validation
inTrain <- createDataPartition(y = data_training_clean$classe, p = 0.7, list = FALSE)
training <- data_training_clean[inTrain, ]
crossval <- data_training_clean[-inTrain, ]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(ggplot2)
library(caret)
ncol(training)
which(sapply(adData,class)=="factor")
summary(training$diagnosis)
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
(cumsum(p$sdev) / sum(p$sdev))[8]
#Result here
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.9)
library(ggplot2)
library(caret)
ncol(training)
which(sapply(adData,class)=="factor")
summary(training$diagnosis)
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
(cumsum(p$sdev) / sum(p$sdev))[8]
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.9)
preProc
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2
install.packages("swirl")
library(swril)
library(swirl)
rm(list=ls())
install_from_swirl("Getting and Cleaning Data")
rm(list=ls())
install.packages("rmarkdown")
install.packages("rmarkdown")
library(rmarkdown)
install.packages('stringr')
install.packages("stringr")
library(stringr)
sessionInfo()
install.packages("installr")
installr::updateR()
install.packages('stringr')
install.packages("stringr")
sessionInfo()
install.packages("devtools")
library(devtools)
find_rtools()
session_info()
library(rmarkdown)
library(knitr)
install.packages('stringr')
install.packages("stringr")
install.packages("stringr")
devtools::install_github('muschellij2/slidify')
library(slidify)
setwd("~/sample_slidify/first")
browseURL('index.Rmd')
slidify('index.Rmd')
browseURL('index.html')
# load all libraries necessary for analysis
library(knitr)
library(psych)
library(ggplot2)
# set global options for code chunks
opts_chunk$set(tidy = TRUE,
fig.width = 7)
# set width so describe() output does not wrap
options(width=120)
# seed value for random number generators
my_seed <- 666
# generate 1000 exponential RVs
set.seed(my_seed)
lambda <- 0.2
sample_size <- 1000
exp_sample <- rexp(sample_size,lambda)
sample_data <- data.frame(exp_sample)
# print some summary statistics for the random exponentials
describe(exp_sample)
install.packages("psych")
library(psych)
# print some summary statistics for the random exponentials
describe(exp_sample)
# generate 1000 samples of 40 exponential RVs
set.seed(my_seed)
lambda <- 0.2
number_samples <- 1000
sample_size <- 40
sample_means = NULL
for (i in 1 : number_samples) sample_means = c(sample_means, mean(rexp(sample_size,lambda)))
# ggplot2 would like a data frame rather than a numeric vector
sample_data <- data.frame(sample_means)
# draw frequency / density histograms to see the shape of the data
plot1 <- ggplot(sample_data,
aes(x=sample_means)) +
xlab("Exponential RV (Sample Means)") +
geom_histogram(colour = "black",
fill = "mediumpurple1",
binwidth = 0.25) +
geom_vline(colour = "darkorange",
xintercept = 5,
size=1) +
theme_bw()
plot1
# print some summary statistics for the collection of sample means
describe(sample_means)
# load the data
data(ToothGrowth)
# summarize data on tooth length by supplement type
by(ToothGrowth$len,
ToothGrowth$supp,
describe)
# OJ regime associated with longer teeth
ggplot(ToothGrowth,
aes(x=len)) +
xlab("Tooth length") +
geom_histogram(colour = "black",
binwidth = 2,
fill="lightblue") +
theme_bw() +
facet_grid(supp ~ .)
# summarize data on tooth length by supplement type
by(ToothGrowth$len,
ToothGrowth$dose,
describe)
# create a factor using dose
ToothGrowth$dose_factor <- factor(ToothGrowth$dose,
levels = c(0.5,1,2),
labels = c("Low0.5","Medium1","High2"),
ordered = TRUE)
# Higher doses associated with longer teeth
ggplot(ToothGrowth,
aes(x=len)) +
xlab("Tooth length") +
geom_histogram(colour = "black",
binwidth = 2,
fill="mediumpurple1") +
theme_bw() +
facet_grid(dose_factor ~ .)
# Supplement type; is there a significant difference in tooth length between the supplement groups?
oj <- subset(ToothGrowth, ToothGrowth$supp == "OJ")
vc <- subset(ToothGrowth, ToothGrowth$supp == "VC")
t.test(oj$len,
vc$len,
var.equal = FALSE,
paired = FALSE)
# we'll compare 2 dosage gropus, low and high, where we're most likely to detect a difference
low_dose <- subset(ToothGrowth, ToothGrowth$dose == 0.5)
high_dose <- subset(ToothGrowth, ToothGrowth$dose == 2)
t.test(high_dose$len,
low_dose$len,
var.equal = FALSE,
paired = FALSE)
install.packages("rmarkdown")
install.packages("rmarkdown")
setwd("~/Sinfr")
render("new.Rmd", "pdf_document")
RMDFILE=example-r-markdown
Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md'); markdownToHTML('$RMDFILE.md', '$RMDFILE.html', options=c('use_xhml'))"
library(knitr)
pandoc('input.md', format = 'latex')
pandoc('new.Rmd', format = 'latex')
setwd("~/reg_analysis")
setwd("~/ddp")
session_info()
author("first_p")
slidify('index.Rmd')
install.packages("shiny")
library(shiny)
runApp()
runApp()
runApp(display.mode = showcase)
runApp(display.mode = 'showcase')
runApp(display.mode = 'showcase')
library(rcharts)
install.packages("rcharts")
slidify('index.Rmd')
slidify('index.Rmd')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
runApp(display.mode = 'showcase')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='sudeep87', token='52A01C91E9E1237E8D92C6E8057F5780', secret='wn8OcT13EEkK7HbqYiJqR3YlYwWPm/JNUJTANp8z')
getwd()
shinyapps::deployApp('C:/Users/MishraJi/Documents/ddp/first_p')
